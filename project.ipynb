{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd045a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "#from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "sys.path.append('../..')\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-PCfh6KnbKIihSwakajqgT3BlbkFJuutFCLsB3pH5h5Rjmskj\"\n",
    "openai.api_key  = os.environ[\"OPENAI_API_KEY\"] \n",
    "\n",
    "## Model declare\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "#llm_model = \"gpt-4\"\n",
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308363ad",
   "metadata": {},
   "source": [
    "# Vector database And RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c880eb61",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************mskj. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m ans \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39msimilarity_search(question,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ans = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#pp.pprint(ans[0].page_content)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m ans2 \u001b[38;5;241m=\u001b[39m \u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_marginal_relevance_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:561\u001b[0m, in \u001b[0;36mChroma.max_marginal_relevance_search\u001b[0;34m(self, query, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor MMR search, you must specify an embedding function on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m     )\n\u001b[0;32m--> 561\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_marginal_relevance_search_by_vector(\n\u001b[1;32m    563\u001b[0m     embedding,\n\u001b[1;32m    564\u001b[0m     k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    569\u001b[0m )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_openai/embeddings/base.py:576\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_openai/embeddings/base.py:535\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_openai/embeddings/base.py:430\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    428\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 430\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    434\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/openai/resources/embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.9/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************mskj. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = []\n",
    "df = pd.read_csv(\"./data/filtered_sd_housing.csv\")\n",
    "for idx, row in df.iterrows():\n",
    "    page_content = f\"Neighborhood:{row['Neighborhood']}\\n\\\n",
    "Price: {row['Price']}\\n\\\n",
    "Bedrooms: {row['Bedrooms']}\\n\\\n",
    "Bathrooms: {row['Bathrooms']}\\n\\\n",
    "House Size:{row['House Size']} \\n\\\n",
    "Location:{row['Location']}\\n \\\n",
    "Phone Number:{row['Phone Number']}\\n\\\n",
    "Construction Year:{row['Construction Year']}\\n\\\n",
    "Neighborhood Description: {row['Neighborhood Description']}\"\n",
    "    metadata = {\"neighborhood\": row[\"Neighborhood\"], \"price\": row[\"Price\"]}\n",
    "    doc = Document(page_content=page_content)\n",
    "    docs.append(doc)\n",
    "\n",
    "# Split\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "persist_directory = 'data/chroma/'\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "# vectordb.persist()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "question = \"Which houses are located at Height\"\n",
    "ans = vectordb.similarity_search(question,k=3)\n",
    "# ans = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "#pp.pprint(ans[0].page_content)\n",
    "ans2 = vectordb.max_marginal_relevance_search(question,k=5, fetch_k=3)\n",
    "#pp.pprint(ans2[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74645512",
   "metadata": {},
   "source": [
    "# Tagging and Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6650bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.3.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neg', 'language': 'it'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a content moderation assistant. Your task is to analyze user messages and determine if they contain any inappropriate, offensive, or harmful content. This includes hate speech, explicit or suggestive content, self-harm, illegal activities, or other sensitive topics. \\\n",
    "     Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "model_with_tagging = llm.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "tagging_chain = prompt | model_with_tagging | JsonOutputFunctionsParser()\n",
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db0a39",
   "metadata": {},
   "source": [
    "# Design Collect Preferences prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ba3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain, ConversationalRetrievalChain\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "total_answers = 'A comfortable 3 bedrooms house with a spacious kitchen and a cozy living room. A quiet neighborhood, good local schools, and convenient shopping options. A backyard for gardening, a two-car garage, and a modern, energy-efficient heating system. Easy access to a reliable bus line, proximity to a major highway, and bike-friendly roads. A balance between suburban tranquility and access to urban amenities like restaurants and theaters.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852af719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build promptr\n",
    "buyer_preferences_template = \"\"\"\n",
    "Conversation History:\n",
    "{conversation_history}\n",
    "\n",
    "You are a professional and polite real estate agent assistant. Your task is to collect buyer preferences for a property by asking\\\n",
    "all the defaults question bellow. \n",
    "REMEMBER: Do not make up the user input. Wait for the user response\n",
    "The DEFAULT questions are:\n",
    "1. How big do you want your house to be?\n",
    "2. What are 3 most important things for you in choosing this property?\n",
    "3. Which amenities would you like?\n",
    "4. Which transportation options are important to you?\n",
    "5. How urban do you want your neighborhood to be?  \n",
    "\n",
    "if the user answers all questions from conversation history,\n",
    "summarize in detail the user inputs and formating it as the json form bellow.\n",
    "Always say \"The buyer is finding\" at the beginning of the summary.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"next_inputs\": string \\ summarize of the user inputs\n",
    "```\n",
    "REMEMBER: only formating the json object only when all buyer information preferences is collected from conversation history.\n",
    "Use three sentences maximum. Keep the answer as concise as possible.\n",
    "User Input:\n",
    "{input}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_buyer_preferences_template =\"\"\"\n",
    "conversation history :{conversation_history}\n",
    "**You are a professional real estate agent and you're meeting with a potential buyer. Your goal is to understand their needs and preferences to find them the perfect home.**\n",
    "\n",
    "**Here are some questions you can ask the buyer:**\n",
    "\n",
    "* **Can you tell me a little bit about yourself and what brings you to the market for a new home?**\n",
    "* **How many bedrooms and bathrooms are you looking for?**\n",
    "* **Do you have a preferred location or area in mind?**\n",
    "* **What is your budget for a new home?**\n",
    "* **What type of property are you interested in (e.g., single-family home, condo, townhouse)?**\n",
    "* **Are there any specific features or amenities that are important to you (e.g., a pool, a garage, a yard)?**\n",
    "* **Do you have a preferred move-in timeframe?**\n",
    "* **Is there anything else you would like to tell me about your ideal home?**\n",
    "\n",
    "**After asking all of the above questions, format the buyer's responses as a JSON object with the following keys:**\n",
    "\n",
    "* **next_input:** Summarize the answers of the buyer. It must start with \"the buyer want to find\". Use at max 4 sentences for the summary\n",
    "**Here is an example of the formatted JSON object:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"next_input\": \"The buyer seeks a comfortable 3-bedroom house with a spacious kitchen and cozy living room. Located in a quiet neighborhood with good schools and shopping, it should also offer a backyard, a two-car garage, and easy access to transportation and amenities. This ideal home combines suburban peace with city convenience.\"\n",
    "}\n",
    "\n",
    "user input:{input}\n",
    "\n",
    "\"\"\"\n",
    "#says something like \"i do not have any more questions\" or \"thank you\" or \"good bye\"\n",
    "qa_template = \"\"\"You are a friendly and professional real estate agent.\n",
    "Use the following pieces of context to find the best home match for a buyer at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "REMEMBER: Check the chat history to provide context-aware answers to the user's questions.\n",
    "If the user want to stop the conversation by saying for example \"i do not have any more questions\" or \"thank you\" or \"good bye\", you must say thank you for asking me at the end.\n",
    "Context: {context}\n",
    "\n",
    "Chat History:{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "def createBuyerPreferencesAgent(buyer_preferences_template,llm_model = \"gpt-3.5-turbo\", temperature = 0):\n",
    "    \"\"\"\n",
    "    Creates an agent for collecting buyer preferences.\n",
    "\n",
    "    Args:\n",
    "        buyer_preferences_template (str): The prompt template for buyer preference questions.\n",
    "\n",
    "    Returns:\n",
    "        ConversationChain: The ConversationChain agent for collecting buyer preferences.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"conversation_history\", \"input\"],\n",
    "        template=buyer_preferences_template\n",
    "    )\n",
    "\n",
    "    conv_memory = ConversationBufferMemory(memory_key=\"conversation_history\")\n",
    "\n",
    "    # Create a ConversationChain with memory and the prompt template\n",
    "    conv_llm = ChatOpenAI(model_name=llm_model, temperature=temperature)\n",
    "    preferences_collected_agent = ConversationChain(\n",
    "        llm=conv_llm,\n",
    "        memory=conv_memory,\n",
    "        prompt=prompt_template,\n",
    "        verbose=False\n",
    "    )\n",
    "    return preferences_collected_agent\n",
    "\n",
    "# Build prompt\n",
    "qa_template = \"\"\"You are a friendly and professional real estate agent.\n",
    "Use the following pieces of context to find the best home match for a buyer at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "REMEMBER: Check the chat history to provide context-aware answers to the user's questions.\n",
    "If the user says something like \"i do not have any more questions\" or \"thank you\" or \"good bye\", you must say thank you for asking me at the end.\n",
    "Context: {context}\n",
    "\n",
    "Chat History:{chat_history}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "def createQADataAgent(qa_template, vectordb, llm_model = \"gpt-3.5-turbo\", temperature = 0):\n",
    "    \"\"\"\n",
    "    Creates an agent for answering questions based on a data source.\n",
    "\n",
    "    Args:\n",
    "        qa_template (str): The prompt template for asking and combining information.\n",
    "\n",
    "    Returns:\n",
    "        ConversationalRetrievalChain: The ConversationalRetrievalChain agent for answering questions.\n",
    "    \"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(qa_template)\n",
    "\n",
    "    # Set up memory\n",
    "    memory_data = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"question\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "    # Set up LLM\n",
    "    qa_llm = ChatOpenAI(model_name=llm_model, temperature= temperature)\n",
    "\n",
    "    # Create the ConversationalRetrievalChain\n",
    "    qa_data_agent = ConversationalRetrievalChain.from_llm(\n",
    "        llm=qa_llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        memory=memory_data,\n",
    "        combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        verbose = False\n",
    "    )\n",
    "    return qa_data_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d62bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A comfortable 3 bedrooms house with a spacious kitchen and a cozy living room. A quiet neighborhood, good local schools, and convenient shopping options. A backyard for gardening, a two-car garage, and a modern, energy-efficient heating system. Easy access to a reliable bus line, proximity to a major highway, and bike-friendly roads. A balance between suburban tranquility and access to urban amenities like restaurants and theaters.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# class BuyerChatStateMachine:\n",
    "#     \"\"\"\n",
    "#     A state machine that manages a conversation with a buyer to collect preferences and find matching properties.\n",
    "\n",
    "#     This class implements a state machine with three states:\n",
    "#     1. collecting_preferences: Gather buyer preferences\n",
    "#     2. finding_match: Search for properties matching the buyer's preferences\n",
    "#     3. finished: End the conversation\n",
    "\n",
    "#     Attributes:\n",
    "#         preferences_collected_agent: An agent for collecting user preferences.\n",
    "#         qa_data_agent: An agent for querying property data.\n",
    "#         qa_data_conversation: An agent for handling follow-up conversations about properties.\n",
    "#         state (str): The current state of the conversation.\n",
    "#         next_input (str): The next input to be processed.\n",
    "#         max_number_conversation (int): Maximum number of allowed conversation turns.\n",
    "#         conversation_count (int): Current number of conversation turns.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, preferences_collected_agent, qa_data_agent, max_number_conversation=10):\n",
    "#         \"\"\"\n",
    "#         Initialize the BuyerChatStateMachine.\n",
    "\n",
    "#         Args:\n",
    "#             preferences_collected_agent: Agent for collecting user preferences.\n",
    "#             qa_data_agent: Agent for querying property data and follow up question.\n",
    "#             qa_data_conversation: Agent for handling follow-up property conversations.\n",
    "#             max_number_conversation (int, optional): Maximum number of conversation turns. Defaults to 10.\n",
    "#         \"\"\"\n",
    "#         self.preferences_collected_agent = preferences_collected_agent\n",
    "#         self.qa_data_agent = qa_data_agent\n",
    "#         self.state = \"collecting_preferences\"\n",
    "#         self.next_input = \"\"\n",
    "#         self.max_number_conversation = max_number_conversation\n",
    "#         self.conversation_count = 0\n",
    "\n",
    "#     def run(self):\n",
    "#         \"\"\"\n",
    "#         Run the state machine to manage the buyer conversation.\n",
    "\n",
    "#         This method controls the flow of the conversation, transitioning between states\n",
    "#         and ensuring the conversation doesn't exceed the maximum number of turns.\n",
    "#         \"\"\"\n",
    "#         print(\"Please ask me anything: \")\n",
    "#         while self.conversation_count < self.max_number_conversation:\n",
    "#             self.conversation_count += 1\n",
    "#             if self.state == \"collecting_preferences\":\n",
    "#                 self._handleCollectingPreferences()\n",
    "#             elif self.state == \"finding_match\":\n",
    "#                 self._handleFindingMatch()\n",
    "#             elif self.state == \"finished\":\n",
    "#                 break\n",
    "#             else:\n",
    "#                 print(\"Invalid state encountered\")\n",
    "#                 break\n",
    "\n",
    "#         if self.conversation_count >= self.max_number_conversation:\n",
    "#             print(f\"Reached maximum number of conversations ({self.max_number_conversation}). Ending chat.\")\n",
    "#         else:\n",
    "#             print(\"Chat finished. Thank you!\")\n",
    "\n",
    "#     def _handleCollectingPreferences(self):\n",
    "#         \"\"\"\n",
    "#         Handle the 'collecting_preferences' state.\n",
    "\n",
    "#         Collects user preferences and checks if all required preferences are gathered.\n",
    "#         If so, transitions to the 'finding_match' state.\n",
    "#         \"\"\"\n",
    "#         buyer_preferences = self._collectUserPreferences()\n",
    "#         is_all_collected = self._isAllPreferencesCollected(buyer_preferences)\n",
    "#         if is_all_collected[0]:\n",
    "#             self.state = \"finding_match\"\n",
    "#             self.next_input = is_all_collected[1]\n",
    "#             return\n",
    "#         print(buyer_preferences)\n",
    "#     def _handleFindingMatch(self):\n",
    "#         \"\"\"\n",
    "#         Handle the 'finding_match' state.\n",
    "\n",
    "#         Queries the property database based on collected preferences and presents results to the user.\n",
    "#         If the user is satisfied, transitions to the 'finished' state. Otherwise, prompts for more input.\n",
    "#         \"\"\"\n",
    "#         response_check = self.qa_data_agent({\"question\": self.next_input})\n",
    "#         print(response_check['answer'])\n",
    "#         if \"thank you for asking me\" in response_check['answer'].lower():\n",
    "#             self.state = \"finished\"\n",
    "#             return\n",
    "#         self.next_input = input(f\"User input (Conversation {self.conversation_count}/{self.max_number_conversation}): \")\n",
    "\n",
    "#     def _collectUserPreferences(self):\n",
    "#         \"\"\"\n",
    "#         Collect user preferences using the preferences_collected_agent.\n",
    "\n",
    "#         Returns:\n",
    "#             str: The collected user preferences.\n",
    "#         \"\"\"\n",
    "#         user_input = input(f\"User input (Conversation {self.conversation_count}/{self.max_number_conversation}): \")\n",
    "#         return self.preferences_collected_agent.predict(input=user_input)\n",
    "\n",
    "#     def _isAllPreferencesCollected(self, user_input):\n",
    "#         \"\"\"\n",
    "#         Check if all required user preferences have been collected.\n",
    "\n",
    "#         Args:\n",
    "#             user_input (str): The user's input to be checked.\n",
    "\n",
    "#         Returns:\n",
    "#             list: A list containing a boolean indicating if all preferences are collected,\n",
    "#                   and the next input if available.\n",
    "#         \"\"\"\n",
    "#         pattern = r'\"next_inputs\":\\s*\"([^\"]*)\"'\n",
    "#         match = re.search(pattern, user_input)\n",
    "#         if match:\n",
    "#             return [True, match.group(1)]\n",
    "#         return [False, \"Not Finished\"]\n",
    "\n",
    "# # Usage example:stop\n",
    "# # preferences_collected_agent = ...  # Initialize your agent\n",
    "# # qa_data_agent = ...  # Initialize your agent\n",
    "# # qa_data_conversation = ...  # Initialize your agent\n",
    "# # \n",
    "# # state_machine = BuyerChatStateMachine(preferences_collected_agent, qa_data_agent, qa_data_conversation, max_number_conversation=15)\n",
    "# # state_machine.run()\n",
    "\n",
    "# total_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d53c7e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A comfortable 3 bedrooms house with a spacious kitchen and a cozy living room. A quiet neighborhood, good local schools, and convenient shopping options. A backyard for gardening, a two-car garage, and a modern, energy-efficient heating system. Easy access to a reliable bus line, proximity to a major highway, and bike-friendly roads. A balance between suburban tranquility and access to urban amenities like restaurants and theaters.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class BuyerChatStateMachine:\n",
    "    \"\"\"\n",
    "    A state machine that manages a conversation with a buyer to collect preferences and find matching properties.\n",
    "\n",
    "    This class implements a state machine with three states:\n",
    "    1. collecting_preferences: Gather buyer preferences\n",
    "    2. finding_match: Search for properties matching the buyer's preferences\n",
    "    3. finished: End the conversation\n",
    "\n",
    "    Attributes:\n",
    "        preferences_collected_agent: An agent for collecting user preferences.\n",
    "        qa_data_agent: An agent for querying property data.\n",
    "        qa_data_conversation: An agent for handling follow-up conversations about properties.\n",
    "        state (str): The current state of the conversation.\n",
    "        next_input (str): The next input to be processed.\n",
    "        max_number_conversation (int): Maximum number of allowed conversation turns.\n",
    "        conversation_count (int): Current number of conversation turns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, preferences_collected_agent, qa_data_agent, max_number_conversation=10):\n",
    "        \"\"\"\n",
    "        Initialize the BuyerChatStateMachine.\n",
    "\n",
    "        Args:\n",
    "            preferences_collected_agent: Agent for collecting user preferences.\n",
    "            qa_data_agent: Agent for querying property data and follow up question.\n",
    "            qa_data_conversation: Agent for handling follow-up property conversations.\n",
    "            max_number_conversation (int, optional): Maximum number of conversation turns. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.preferences_collected_agent = preferences_collected_agent\n",
    "        self.qa_data_agent = qa_data_agent\n",
    "        self.state = \"collecting_preferences\"\n",
    "        self.response = \"\"\n",
    "        self.max_number_conversation = max_number_conversation\n",
    "        self.conversation_count = 0\n",
    "        self.rag_first_trigger = True\n",
    "\n",
    "    def generateAnswer(self, user_input):\n",
    "        \"\"\"\n",
    "        Run the state machine to manage the buyer conversation.\n",
    "\n",
    "        This method controls the flow of the conversation, transitioning between states\n",
    "        and ensuring the conversation doesn't exceed the maximum number of turns.\n",
    "        \"\"\"\n",
    "        print(\"Please ask me anything: \")\n",
    "        if (self.conversation_count < self.max_number_conversation):\n",
    "            self.conversation_count += 1\n",
    "            if self.state == \"collecting_preferences\":\n",
    "                self._handleCollectingPreferences(user_input)\n",
    "            elif self.state == \"finding_match\":\n",
    "                self._handleFindingMatch(user_input)\n",
    "            elif self.state == \"finished\":\n",
    "                self.response = \"Finish State Reach\"\n",
    "            else:\n",
    "                self.response = \"Invalid state encountered\"\n",
    "        else:\n",
    "            self.response = f\"Reached maximum number of conversations ({self.max_number_conversation}). Ending chat.\"\n",
    "        # else:\n",
    "        #     print(\"Chat finished. Thank you!\")\n",
    "        return self.response\n",
    "\n",
    "    def _handleCollectingPreferences(self, user_input):\n",
    "        \"\"\"\n",
    "        Handle the 'collecting_preferences' state.\n",
    "\n",
    "        Collects user preferences and checks if all required preferences are gathered.\n",
    "        If so, transitions to the 'finding_match' state.\n",
    "        \"\"\"\n",
    "        self.response = self.preferences_collected_agent.predict(input=user_input)\n",
    "        is_all_collected = self._isAllPreferencesCollected( self.response )\n",
    "        if is_all_collected[0]:\n",
    "            self.state = \"finding_match\"\n",
    "            if(self.rag_first_trigger):\n",
    "                self.response = self.qa_data_agent({\"question\": is_all_collected[1]})['answer']\n",
    "                self.rag_first_trigger = False\n",
    "\n",
    "    def _handleFindingMatch(self,user_input):\n",
    "        \"\"\"\n",
    "        Handle the 'finding_match' state.\n",
    "\n",
    "        Queries the property database based on collected preferences and presents results to the user.\n",
    "        If the user is satisfied, transitions to the 'finished' state. Otherwise, prompts for more input.\n",
    "        \"\"\"\n",
    "        self.response = self.qa_data_agent({\"question\": user_input})['answer']\n",
    "        if \"thank you for asking me\" in self.response.lower():\n",
    "            self.rag_first_trigger = True\n",
    "            self.state = \"finished\"\n",
    "        # self.next_input = input(f\"User input (Conversation {self.conversation_count}/{self.max_number_conversation}): \")\n",
    "\n",
    "\n",
    "    def _isAllPreferencesCollected(self, user_input):\n",
    "        \"\"\"\n",
    "        Check if all required user preferences have been collected.\n",
    "\n",
    "        Args:\n",
    "            user_input (str): The user's input to be checked.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing a boolean indicating if all preferences are collected,\n",
    "                  and the next input if available.\n",
    "        \"\"\"\n",
    "        pattern = r'\"next_inputs\":\\s*\"([^\"]*)\"'\n",
    "        match = re.search(pattern, user_input)\n",
    "        if match:\n",
    "            return [True, match.group(1)]\n",
    "        return [False, \"Not Finished\"]\n",
    "\n",
    "\n",
    "\n",
    "total_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f04fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example:\n",
    "preferences_collected_agent = createBuyerPreferencesAgent(buyer_preferences_template=buyer_preferences_template)  # Initialize your agent\n",
    "qa_data_agent = createQADataAgent(qa_template=qa_template, vectordb=vectordb)  # Initialize your agent\n",
    "home_match_bot = BuyerChatStateMachine(preferences_collected_agent=preferences_collected_agent,qa_data_agent= qa_data_agent, max_number_conversation=10)\n",
    "# home_match_bot.generateAnswer(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99f191a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://18db9002d15c8af954.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 193, in run_endpoint_function\n",
      "    return await run_in_threadpool(dependant.call, **values)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/concurrency.py\", line 42, in run_in_threadpool\n",
      "    return await anyio.to_thread.run_sync(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/routes.py\", line 291, in api_info\n",
      "    return gradio.blocks.get_api_info(config, serialize)  # type: ignore\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/blocks.py\", line 528, in get_api_info\n",
      "    serializer = serializing.COMPONENT_MAPPING[type]()\n",
      "KeyError: 'dataset'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://18db9002d15c8af954.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 193, in run_endpoint_function\n",
      "    return await run_in_threadpool(dependant.call, **values)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/concurrency.py\", line 42, in run_in_threadpool\n",
      "    return await anyio.to_thread.run_sync(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/routes.py\", line 291, in api_info\n",
      "    return gradio.blocks.get_api_info(config, serialize)  # type: ignore\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/blocks.py\", line 528, in get_api_info\n",
      "    serializer = serializing.COMPONENT_MAPPING[type]()\n",
      "KeyError: 'dataset'\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/applications.py\", line 123, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 756, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 776, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 297, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 77, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/routing.py\", line 72, in app\n",
      "    response = await func(request)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 278, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/fastapi/routing.py\", line 193, in run_endpoint_function\n",
      "    return await run_in_threadpool(dependant.call, **values)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/starlette/concurrency.py\", line 42, in run_in_threadpool\n",
      "    return await anyio.to_thread.run_sync(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/routes.py\", line 291, in api_info\n",
      "    return gradio.blocks.get_api_info(config, serialize)  # type: ignore\n",
      "  File \"/home/noah-u18-alien/anaconda3/envs/langchain/lib/python3.9/site-packages/gradio/blocks.py\", line 528, in get_api_info\n",
      "    serializer = serializing.COMPONENT_MAPPING[type]()\n",
      "KeyError: 'dataset'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# Function to generate answer\n",
    "\n",
    "# Function to generate answer\n",
    "def generateBotAnswer(message, history):\n",
    "    history = history or []\n",
    "    response = home_match_bot.generateAnswer(message)  # Note: Fixed typo in method name\n",
    "    history.append((message, response))\n",
    "    return history, history\n",
    "\n",
    "# # Create Gradio interface\n",
    "# with gr.Blocks() as block:\n",
    "#     gr.Markdown(\"\"\"<h1><center>Home Match Bot</center></h1>\"\"\")\n",
    "#     chatbot = gr.Chatbot()\n",
    "#     message = gr.Textbox(placeholder=\"Ask me a question about houses\")\n",
    "#     state = gr.State([])  # Initialize with an empty list\n",
    "#     submit = gr.Button(\"Send\")\n",
    "    \n",
    "#     # Set up the click event\n",
    "#     submit.click(\n",
    "#         generate_bot_answer, \n",
    "#         inputs=[message, state], \n",
    "#         outputs=[chatbot, state]\n",
    "#     )\n",
    "\n",
    "# # Launch the interface\n",
    "# block.launch(debug=True)\n",
    "\n",
    "# Set up chat bot interface\n",
    "# Check if ChatInterface is available\n",
    "if hasattr(gr, 'ChatInterface'):\n",
    "    # Use ChatInterface for newer Gradio versions\n",
    "    answer_bot = gr.ChatInterface(\n",
    "        generateBotAnswer,\n",
    "        chatbot=gr.Chatbot(height=300),\n",
    "        textbox=gr.Textbox(placeholder=\"Ask me a question about finding home\", container=False, scale=7),\n",
    "        title=\"Perfect Home Match ChatBot\",\n",
    "        description=\"Ask The Real Estate Agent!\",\n",
    "        theme=\"soft\",\n",
    "        examples=[\"location\", \"neighborhood\", \"price range\", \"house size\", \"contact\"],\n",
    "        cache_examples=False,\n",
    "        retry_btn=None,\n",
    "        undo_btn=None,\n",
    "        clear_btn=None,\n",
    "        submit_btn=\"Ask\"\n",
    "    )\n",
    "else:\n",
    "    # Fallback for older Gradio versions\n",
    "    with gr.Blocks(theme=\"soft\") as answer_bot:\n",
    "        gr.Markdown(\"# Perfect Home Match ChatBot\")\n",
    "        gr.Markdown(\"Ask The Real Estate Agent!\")\n",
    "        \n",
    "        chatbot = gr.Chatbot(height=300)\n",
    "        msg = gr.Textbox(placeholder=\"Ask me a question about finding home\", container=False, scale=7)\n",
    "        \n",
    "        with gr.Row():\n",
    "            submit = gr.Button(\"Ask\")\n",
    "        \n",
    "        examples = gr.Examples(\n",
    "            examples=[\"location\", \"neighborhood\", \"price range\", \"house size\", \"contact\"],\n",
    "            inputs=msg\n",
    "        )\n",
    "        \n",
    "        def respond(message, chat_history):\n",
    "            bot_message = generateBotAnswer(message, chat_history)\n",
    "            chat_history.append((message, bot_message))\n",
    "            return \"\", chat_history\n",
    "        \n",
    "        submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "# Launch the interface\n",
    "answer_bot.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c8b1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A comfortable 3 bedrooms house with a spacious kitchen and a cozy living room. A quiet neighborhood, good local schools, and convenient shopping options. A backyard for gardening, a two-car garage, and a modern, energy-efficient heating system. Easy access to a reliable bus line, proximity to a major highway, and bike-friendly roads. A balance between suburban tranquility and access to urban amenities like restaurants and theaters.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "745e25ac",
   "metadata": {},
   "source": [
    "# Tools and Routing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aaf983bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The best home match for a family of four would be this charming single-story 4 bedroom, 2 bathroom home located at 5045 Park West Ave in the highly sought-after neighborhood of Clairemont Mesa, San Diego, CA 92117. This home is a blank canvas waiting for you to make it your own, with a newer roof, a spacious lot, and the opportunity to customize to your liking. The neighborhood offers easy access to amenities like bars, restaurants, coffee shops, and parks, making it a convenient and enjoyable place to live. Priced at $1,093,000, this home is a must-see for any family looking for a cozy and welcoming space in a great location. Professional pictures and video are coming soon. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# Build prompts\n",
    "question_prompt_template = \"\"\"Use the following pieces of context to find the best home match for a buyer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "{context_str}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QUESTION_PROMPT = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"context_str\", \"question\"]\n",
    ")\n",
    "\n",
    "refine_template = \"\"\"The original question is as follows: {question}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer\n",
    "(only if needed) with some more context below.\n",
    "------------\n",
    "{context_str}\n",
    "------------\n",
    "Given the new context, refine the original answer to better answer the question. \n",
    "If the context isn't useful, return the original answer. Always say \"thanks for asking!\" at the end of the answer.\"\"\"\n",
    "REFINE_PROMPT = PromptTemplate(\n",
    "    template=refine_template,\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    ")\n",
    "\n",
    "# Create a prompt for question generation\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "memory_data = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create the refine chain\n",
    "refine_chain = load_qa_chain(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=QUESTION_PROMPT,\n",
    "    refine_prompt=REFINE_PROMPT,\n",
    ")\n",
    "\n",
    "# Create a question generator chain\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain(\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=memory_data,\n",
    "    combine_docs_chain=refine_chain,\n",
    "    question_generator=question_generator,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# To use the chain:\n",
    "query = \"What's the best home match for a family of four?\"\n",
    "result = qa_chain({\"question\": query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86da901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "5381 Vergara St, San Diego, CA 92117\n",
      "\n",
      "Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "# To use the chain:\n",
    "query = \"What's the address of this house?\"\n",
    "result = qa_chain({\"question\": query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6d191f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'destinations_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m\n\u001b[1;32m      1\u001b[0m MULTI_PROMPT_ROUTER_TEMPLATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven a raw text input to a \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mlanguage model select the model prompt best suited for the input. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mYou will be given the names of the available prompts and a \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m<< OUTPUT (remember to include the ```json)>>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     32\u001b[0m router_template \u001b[38;5;241m=\u001b[39m MULTI_PROMPT_ROUTER_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 33\u001b[0m     destinations\u001b[38;5;241m=\u001b[39m\u001b[43mdestinations_str\u001b[49m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m router_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     36\u001b[0m     template\u001b[38;5;241m=\u001b[39mrouter_template,\n\u001b[1;32m     37\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m     output_parser\u001b[38;5;241m=\u001b[39mRouterOutputParser(),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m router_chain \u001b[38;5;241m=\u001b[39m LLMRouterChain\u001b[38;5;241m.\u001b[39mfrom_llm(llm, router_prompt, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'destinations_str' is not defined"
     ]
    }
   ],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{user_input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
    "\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"user_input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt, verbose = True)\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )\n",
    "\n",
    "buyer_preferences_template = \"\"\"You are a very professional real estate agent assistant. \\\n",
    "Your task is to collect best matches for a property from user preferences.\n",
    "try to make it clear to call the database\\\n",
    "\"\"\"\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"buyer_preferences_finding\", \n",
    "        \"description\": \"Finding match home from byer preferences\", \n",
    "        \"prompt_template\": buyer_preferences_template\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, memory= ConversationBufferMemory())\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{user_input}\")\n",
    "default_memory = ConversationBufferMemory()\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt, memory=default_memory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
